{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d61dae",
   "metadata": {},
   "source": [
    "#### Hybrid Retriever - Combining Dense & Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b27fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e580344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# STep2: Dense Retriever (FAISSS + HUggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever=dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9747dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sparse Retriever (BM25)\n",
    "sparse_retriever=BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3 ##top-k documents to retriever\n",
    "\n",
    "## Step 4: Combine with ensemble retriever\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weight=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9146007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E859917890>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001E8599179D0>, k=3)], weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03d1e5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Docuement: 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      " Docuement: 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      " Docuement: 3:\n",
      "Langchain has many types of retrievers.\n",
      "\n",
      " Docuement: 4:\n",
      "Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step5: Query & get results\n",
    "query=\"How can I build an application using LLM\"\n",
    "results=hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step6: Print results\n",
    "for i,doc in enumerate(results):\n",
    "    print(f\"\\n Docuement: {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef538190",
   "metadata": {},
   "source": [
    "#### RAG PipleLine with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "001d0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5b6f98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7c1abec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E859AE9E00>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E859AEA190>, root_client=<openai.OpenAI object at 0x000001E859AE9BA0>, root_async_client=<openai.AsyncOpenAI object at 0x000001E859AE9F30>, model_name='openai/gpt-oss-120b', temperature=0.6, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STep[5]: PromptTemplate\n",
    "prompt=PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "## Step[6] - LLM\n",
    "openAILLM = (\"openai/gpt-oss-120b\")\n",
    "llm = init_chat_model(\n",
    "    model=openAILLM,\n",
    "    model_provider=\"openai\",\n",
    "    api_key=os.getenv('GROQ_API_KEY'),\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f296ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E859917890>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001E8599179D0>, k=3)], weights=[0.5, 0.5]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E859AE9E00>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E859AEA190>, root_client=<openai.OpenAI object at 0x000001E859AE9BA0>, root_async_client=<openai.AsyncOpenAI object at 0x000001E859AE9F30>, model_name='openai/gpt-oss-120b', temperature=0.6, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create stuff Document Chain\n",
    "document_chain=create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "## Create FULL RAG chain\n",
    "rag_chain=create_retrieval_chain(\n",
    "    retriever=hybrid_retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")\n",
    "\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84316d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Below is a practical, end‚Äëto‚Äëend roadmap you can follow to turn a large language model (LLM) into a working application.  \n",
      "I‚Äôll frame each step around the tools you mentioned‚Äî**LangChain**, its **retrievers**, and **Pinecone**‚Äîbut the same pattern works with other LLM providers, vector stores, or deployment platforms.\n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£ Define the problem & the user experience\n",
      "\n",
      "| Question | Why it matters |\n",
      "|----------|----------------|\n",
      "| **What does the app do?** (e.g., answer FAQs, generate code, act as a personal assistant) | Determines the prompt design, required data, and whether you need an ‚Äúagentic‚Äù (tool‚Äëusing) workflow. |\n",
      "| **Who are the users?** (internal staff, public customers, developers) | Drives UI choices (web UI, Slack bot, API, etc.) and security requirements. |\n",
      "| **What latency / cost constraints do you have?** | Influences model selection (e.g., GPT‚Äë4 vs. a smaller open‚Äësource model) and how much you cache or pre‚Äëcompute. |\n",
      "\n",
      "*Write a one‚Äësentence product vision.*  \n",
      "> ‚ÄúA chat‚Äëbased knowledge‚Äëassistant that can answer company‚Äëpolicy questions using our internal docs, and, when needed, call the HR ticketing system to create a request.‚Äù\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£ Pick an LLM provider & set up credentials\n",
      "\n",
      "```python\n",
      "# Example with OpenAI (you can swap in Anthropic, Cohere, HuggingFace, etc.)\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-‚Ä¶\"\n",
      "```\n",
      "\n",
      "- **OpenAI** ‚Üí `gpt-4o`, `gpt-3.5-turbo`  \n",
      "- **Anthropic** ‚Üí `claude-3-sonnet`  \n",
      "- **Local** ‚Üí Llama‚Äë3, Mistral, etc. (requires a local inference server)\n",
      "\n",
      "Choose the model that balances performance and cost for your use‚Äëcase.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£ Install LangChain (the glue that connects LLMs, tools, and data)\n",
      "\n",
      "```bash\n",
      "pip install \"langchain[all]\"   # pulls in LLM wrappers, agents, retrievers, etc.\n",
      "```\n",
      "\n",
      "LangChain gives you:\n",
      "\n",
      "| Feature | What it solves |\n",
      "|---------|----------------|\n",
      "| **LLM wrappers** | Uniform API for any provider. |\n",
      "| **Chains** | Sequence of prompts, post‚Äëprocessing, or tool calls. |\n",
      "| **Agents** | Dynamic decision‚Äëmaking (choose a tool, re‚Äëprompt). |\n",
      "| **Retrievers** | Pull relevant documents from a vector store (or other sources). |\n",
      "| **Memory** | Keep conversation context across turns. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£ Build a knowledge base (optional but common)\n",
      "\n",
      "If your app needs *grounded* answers (e.g., company policies, product manuals), store the source texts in a vector database.\n",
      "\n",
      "### 4.1 Prepare the documents\n",
      "```python\n",
      "from pathlib import Path\n",
      "docs = []\n",
      "for file in Path(\"docs/\").rglob(\"*.md\"):\n",
      "    docs.append({\"content\": file.read_text(), \"metadata\": {\"source\": str(file)}})\n",
      "```\n",
      "\n",
      "### 4.2 Embed the texts\n",
      "```python\n",
      "from langchain.embeddings import OpenAIEmbeddings   # or HuggingFaceEmbeddings\n",
      "embedder = OpenAIEmbeddings()   # uses the same API key as the LLM\n",
      "```\n",
      "\n",
      "### 4.3 Push embeddings to Pinecone (or any other vector DB)\n",
      "\n",
      "```python\n",
      "import pinecone, uuid\n",
      "\n",
      "pinecone.init(api_key=\"YOUR_PINECONE_KEY\", environment=\"us-west1-gcp\")\n",
      "index = pinecone.Index(\"my-app-knowledge\")\n",
      "\n",
      "vectors = []\n",
      "for doc in docs:\n",
      "    vec = embedder.embed_query(doc[\"content\"])\n",
      "    vectors.append((str(uuid.uuid4()), vec, doc[\"metadata\"]))\n",
      "\n",
      "# Upsert in batches (max 100 per request)\n",
      "index.upsert(vectors=vectors[:100])\n",
      "```\n",
      "\n",
      "### 4.4 Create a LangChain Retriever that talks to Pinecone\n",
      "```python\n",
      "from langchain.vectorstores import Pinecone\n",
      "from langchain.retrievers import PineconeRetriever\n",
      "\n",
      "vectorstore = Pinecone(index, embedder.embed_query, \"metadata\")\n",
      "retriever = PineconeRetriever(vectorstore=vectorstore, search_kwargs={\"k\": 4})\n",
      "```\n",
      "\n",
      "Now `retriever.get_relevant_documents(\"What is the vacation policy?\")` will return the top‚Äë4 most similar chunks.\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£ Wire the LLM + Retriever into a **Chain** (or **Agent**)\n",
      "\n",
      "### 5.1 Simple Retrieval‚ÄëAugmented Generation (RAG) chain\n",
      "```python\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import RetrievalQA\n",
      "\n",
      "prompt = PromptTemplate(\n",
      "    template=\"\"\"\n",
      "You are a helpful company policy assistant. Use ONLY the provided context to answer the question.\n",
      "If the context does not contain the answer, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "\"\"\",\n",
      "    input_variables=[\"context\", \"question\"],\n",
      ")\n",
      "\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
      "    retriever=retriever,\n",
      "    chain_type=\"stuff\",          # \"stuff\" = concat all docs; alternatives: \"map_reduce\", \"refine\"\n",
      "    return_source_documents=True,\n",
      "    combine_prompt=prompt,\n",
      ")\n",
      "```\n",
      "\n",
      "**Usage**\n",
      "\n",
      "```python\n",
      "result = qa_chain({\"query\": \"How many PTO days do I get each year?\"})\n",
      "print(result[\"answer\"])\n",
      "print(\"Sources:\", [doc.metadata[\"source\"] for doc in result[\"source_documents\"]])\n",
      "```\n",
      "\n",
      "### 5.2 Agentic workflow (when you need tools)\n",
      "\n",
      "If the app must *act* (e.g., create a ticket, call an internal API), use a LangChain **Agent**:\n",
      "\n",
      "```python\n",
      "from langchain.tools import tool\n",
      "from langchain.agents import initialize_agent, AgentType\n",
      "\n",
      "# Example tool: create a ticket in an internal system\n",
      "@tool\n",
      "def create_hr_ticket(employee_id: str, issue: str) -> str:\n",
      "    \"\"\"Creates an HR ticket and returns the ticket ID.\"\"\"\n",
      "    # (real implementation would call your internal service)\n",
      "    return f\"TICKET-{employee_id[:4].upper()}-{int(time.time())}\"\n",
      "\n",
      "tools = [create_hr_ticket]\n",
      "\n",
      "agent = initialize_agent(\n",
      "    tools,\n",
      "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
      "    agent_type=AgentType.OPENAI_FUNCTIONS,   # uses OpenAI function calling\n",
      "    verbose=True,\n",
      ")\n",
      "\n",
      "# Example interaction\n",
      "agent.run(\"John Doe needs a new laptop because his old one broke.\")\n",
      "```\n",
      "\n",
      "The agent decides whether to just answer from the knowledge base or to invoke `create_hr_ticket`.\n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£ Build the user‚Äëfacing interface\n",
      "\n",
      "| Option | When it fits |\n",
      "|--------|--------------|\n",
      "| **FastAPI / Flask** | Simple HTTP API, easy to containerize. |\n",
      "| **Streamlit / Gradio** | Quick UI for prototypes (drag‚Äëand‚Äëdrop). |\n",
      "| **React / Next.js** | Full‚Äëfeatured web app. |\n",
      "| **Slack / Discord bots** | Internal team tools. |\n",
      "| **Serverless (AWS Lambda, Cloudflare Workers)** | Low‚Äëtraffic, cost‚Äëeffective. |\n",
      "\n",
      "### Minimal FastAPI example\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, Body\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class Query(BaseModel):\n",
      "    question: str\n",
      "\n",
      "@app.post(\"/ask\")\n",
      "async def ask(query: Query):\n",
      "    result = qa_chain({\"query\": query.question})\n",
      "    return {\n",
      "        \"answer\": result[\"answer\"],\n",
      "        \"sources\": [doc.metadata[\"source\"] for doc in result[\"source_documents\"]],\n",
      "    }\n",
      "```\n",
      "\n",
      "Run with `uvicorn app:app --reload` and you have a JSON endpoint you can call from any front‚Äëend.\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£ Add **memory** (optional but essential for chat‚Äëstyle apps)\n",
      "\n",
      "```python\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "\n",
      "chat_chain = ConversationalRetrievalChain.from_llm(\n",
      "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
      "    retriever=retriever,\n",
      "    memory=memory,\n",
      ")\n",
      "```\n",
      "\n",
      "Now each turn automatically includes the prior dialogue, giving a more natural conversational experience.\n",
      "\n",
      "---\n",
      "\n",
      "## 8Ô∏è‚É£ Test, iterate, and monitor\n",
      "\n",
      "| Checklist |\n",
      "|-----------|\n",
      "| **Unit tests** for each chain (mock the LLM if you want deterministic tests). |\n",
      "| **Prompt quality** ‚Äì iterate on the system prompt until answers are reliable. |\n",
      "| **Safety** ‚Äì add content filters or ‚Äúguardrails‚Äù (e.g., LangChain‚Äôs `LLMGuard` or OpenAI moderation). |\n",
      "| **Cost tracking** ‚Äì log token usage (`response.usage`) and set a budget alert. |\n",
      "| **Observability** ‚Äì instrument request latency, error rates, and vector‚Äësearch hit‚Äërates (Pinecone provides metrics). |\n",
      "| **Versioning** ‚Äì keep the prompt, schema, and code in Git; tag releases. |\n",
      "\n",
      "---\n",
      "\n",
      "## 9Ô∏è‚É£ Deploy & scale\n",
      "\n",
      "| Platform | Typical steps |\n",
      "|----------|---------------|\n",
      "| **Docker + Kubernetes** | Containerize the FastAPI app, add a sidecar for Pinecone credentials, configure autoscaling. |\n",
      "| **Serverless (AWS Lambda + API Gateway)** | Package the code, set environment variables, use a managed VPC to reach Pinecone. |\n",
      "| **Managed LangChain Cloud** (if available) | Push the chain to LangChain‚Äôs hosted service ‚Äì it handles scaling and monitoring for you. |\n",
      "| **Edge (Vercel, Cloudflare Workers)** | Good for low‚Äëlatency UI only; keep the heavy LLM calls on a backend. |\n",
      "\n",
      "Don‚Äôt forget to **secure** the endpoint (API keys, OAuth, rate limiting) especially if the app accesses internal documents.\n",
      "\n",
      "---\n",
      "\n",
      "## üì¶ TL;DR ‚Äì Minimal Working Skeleton\n",
      "\n",
      "```python\n",
      "# 1Ô∏è‚É£ Install\n",
      "# pip install \"langchain[all]\" pinecone-client fastapi uvicorn\n",
      "\n",
      "# 2Ô∏è‚É£ Set env vars\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-‚Ä¶\"\n",
      "os.environ[\"PINECONE_API_KEY\"] = \"‚Ä¶\"\n",
      "\n",
      "# 3Ô∏è‚É£ Build vector store (run once)\n",
      "# ‚Ä¶ (embed docs, upsert to Pinecone) ‚Ä¶\n",
      "\n",
      "# 4Ô∏è‚É£ Create retriever + RAG chain\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Pinecone\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "embedder = OpenAIEmbeddings()\n",
      "vectorstore = Pinecone(pinecone.Index(\"my-app-knowledge\"), embedder.embed_query, \"metadata\")\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "\n",
      "prompt = PromptTemplate(\n",
      "    template=\"\"\"\n",
      "You are a helpful assistant. Answer using ONLY the context below.\n",
      "Context:\n",
      "{context}\n",
      "Question: {question}\n",
      "\"\"\",\n",
      "    input_variables=[\"context\", \"question\"],\n",
      ")\n",
      "\n",
      "qa = RetrievalQA.from_chain_type(\n",
      "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
      "    retriever=retriever,\n",
      "    combine_prompt=prompt,\n",
      "    return_source_documents=True,\n",
      ")\n",
      "\n",
      "# 5Ô∏è‚É£ FastAPI wrapper\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class Q(BaseModel):\n",
      "    question: str\n",
      "\n",
      "@app.post(\"/ask\")\n",
      "def ask(q: Q):\n",
      "    res = qa({\"query\": q.question})\n",
      "    return {\"answer\": res[\"answer\"],\n",
      "            \"sources\": [d.metadata[\"source\"] for d in res[\"source_documents\"]]}\n",
      "```\n",
      "\n",
      "Run:\n",
      "\n",
      "```bash\n",
      "uvicorn myapp:app --host 0.0.0.0 --port 8000\n",
      "```\n",
      "\n",
      "You now have a **complete LLM‚Äëpowered app** that:\n",
      "\n",
      "1. Retrieves relevant knowledge from Pinecone.  \n",
      "2. Generates a grounded answer with LangChain‚Äôs RAG chain.  \n",
      "3. Exposes a clean HTTP endpoint for any front‚Äëend (web, mobile, Slack, etc.).  \n",
      "\n",
      "From here you can add agents, memory, UI, monitoring, and production‚Äëgrade deployment as described in the steps above. Happy building! üöÄ\n",
      "\n",
      " Source Documents\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      "Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "## STep[9] - Ask a question\n",
    "query={\"input\": \"How can I build an app using an LLM?\"}\n",
    "response=rag_chain.invoke(query)\n",
    "\n",
    "## Step 10: Output\n",
    "print(\"Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "\n",
    "print(\"\\n Source Documents\")\n",
    "for i,doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967835d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
